{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53a91211",
   "metadata": {},
   "source": [
    "# Final assigment Supervised Learning\n",
    "\n",
    "Dysphonic disorders can arise from several pathological processes. In parkinsons diseases dysphonia occurs often (70%) due to the affect of the basal ganglia on the laryngeal muscles. Dysphonia massively impacts the quality of life. \n",
    "\n",
    "Parkinsons diseases is caused by the cell death of melanin-containing dopaminergic neurons. These neurons are mainly found in the substantia nigra (brain region in the central brain, between brain stem and bridge). The cell death of the dopaminergic neurons leads to dysfunction of basal ganglia and reduced inhibition of dopaminergic controlled processes (e.g. regulation of laryngeal tension, conotrol of movement muscles). Currently, Parkinsons diseases is diagnosed through the appearance of the trias of symptoms rigidity, tremor and bradykinesia (slowness of movements) and verified by detecting the cell atrophy in the substantia nigra through MRI scanning. \n",
    "\n",
    "Since MRI scanning is an expensive diagnosis technique, alternative approaches are desirable. The previously mentioned dysphonia, arising from the altered laryngeal regulation is measurable via voice recordings. Here, we aim to build a model, which can classify subjects based on voice recordings (and the related features) into Healthy subjects and parkinson patients. Using voice recordings for diagnostics could simplify the diagnosis in several ways:\n",
    "- every GP can make recordings and providing GPs with the model wrapped in a user-friendly software, every GP has a tool to support Parkinson diagnostics \n",
    "- patients could be sent to radiology centers prioritized based on the voice recording classification\n",
    "\n",
    "For the chosen project, a labeled dataset is available (see data source and paper here), which makes it a supervised learning problem. The model will be trained and launched as batch learning model. The evaluation metrics used here are accuracy (to get an impression of the overall accuracy, it is a suitable measure for the models based on balanced dataset generated prior to modelling). Further the confusion matrix is computed. From the confusion matrix especially the False negative predictions are important to check, because it would result in a lot of other diagnostics conducted to find the reason for the dysphonia apart from Parkinson. False positives are more acceptable, because a following MRI scan could verify the diagnosis. Since a lof False negative rate is desired recall measure is an important measure too.\n",
    "\n",
    "In medical diagnostics a gold standard for accuracy is 95%, which is desired to achieve here as well. Moreover, the number of false negative predictions should be very small. Further, recall should be above 80%.\n",
    "\n",
    "The outcome variable is very skewed towards the parkinson class. A model, which would always predict the bigger class (Parkinson disease) would be correct in about 75% of all cases. A balancing of groups seems to be necessary. Splitting the dataset into training and test set could contain nearby no data from the group 0 under unfortunate circumstances. In this project models were trained based on balanced and unbalanced dataset, whereby more attention was paid to the balanced one since this approach appeared to be more promising and realiable.\n",
    "\n",
    "There are two approaches to balance groups: undersampling: reduce samples in the majority class oversampling: increase number of samples in the minority class Undersampling may discard a lot of information while oversampling might be a costly task to conduct. Undersampling could be conducted using TOMEK technique, which picks quite close data points from opposite classes (pairs) and discards the points from each pair which belongs to the majority class. Here this would lead to loss of a lot of data so oversampling is prefered. Here conducted is the SMOTE technique, which measures the distances between the minority class and creates synthetic observations inbetween adjacent observations from the minority class [https://www.analyticsvidhya.com/blog/2020/07/10-techniques-to-deal-with-class-imbalance-in-machine-learning/].\n",
    "\n",
    "Based on balanced dataset, simple classification algorithms as well as ensemble methods were used. From the simple methods logisitic regression and naive bayes were not satisfactory. Supported vector machine approach achieved satisfactory accuracy, false negative rate and recall. The learning curve showed a deacrease of error with more training data with stabilisation of the error for the test dataset. Ensemble methods were applied as well, such as extreme gradient boosting and stacking. The extreme gradient boosting was assumed to perform very well due to it learning from the errors its decision trees are doing and as well coming with builtin regularisation, however it overfitted and hyperparameter tuning was required. The accuracy and fitting can be improved. For the future grid search could be used to find even more optimal parameters than the trial approach conducted here achieved. Stacking of logistic regression model, supported vector machine and naive bayes showed similar performance as supported vector machine alone and extreme gradient boosting however the RMSE stabilized faster and was more similar between training and test set. Support vector machine seems to be here the best option, because it is a fairly simple model and showed good performance without hyperparameter tuning which makes it an quick and easy solution. The extreme gradient boosting could be superior but requires more investment into hyperparameter tuning.\n",
    "\n",
    "Logistic regression, naive bayes, supported vector machine and extreme gradient boosting were as well run on the unbalanced dataset. The performance metrics for logistic regression was better than for the unbalanced dataset, this might be due to predicting almost always the bigger group results in higher performance metrics while the actual performance of the model is not really good. Naive Bayes showed as well better performance for the unbalanced dataset. Support vector machine showed inferior performance to the one trained on balanced dataset. Extreme gradient boosting was slightly less performant, but as well overfitted. Parameter tuning was not conducted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning_env",
   "language": "python",
   "name": "machine_learning_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
